<!DOCTYPE HTML>
<html lang="en"><head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TF5SPSGGYM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TF5SPSGGYM');
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-EZFRNCT6W3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EZFRNCT6W3');
</script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yefan Zhou</title>

  <meta name="author" content="Yefan Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:75%;vertical-align:left">
              <p style="text-align:center">
                <name>Yefan Zhou</name>
              </p>
              <p class="intro">Hi, I'm Yefan. I'm a Master's student in Electrical Engineering and Computer Science 
                at UC Berkeley advised by <a href="https://www.stat.berkeley.edu/~mmahoney/">Michael Mahoney</a>.<br>
                Before that, I received my Bachelor degree from Southeast University, advised by <a href="https://scholar.google.com.hk/citations?user=Bx58-p4AAAAJ&hl=en">Luxi Yang</a>.
              </p>
              <p style="text-align:center">
                <a href="https://github.com/YefanZhou"><i class="fa fa-github" style="font-size:24px"></i></a> &nbsp/&nbsp
                <a href="https://twitter.com/LiamZhou98"><i class="fa fa-twitter" style="font-size:24px"></i></a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yefan-zhou-b430541b1/">Linkedin</a> &nbsp/&nbsp
                <a href="mailto:yefan0726@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/resume_general_feb_16.pdf">Resume</a>
              </p>
            </td>
           <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Yefan.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Yefan-circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in the efficiency and interpretability of machine learning, and 3D Vision.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/3d_ds.png' width="300"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2111.15158">
                <papertitle>A Dataset-dispersion Perspective on Reconstruction versus Recognition in Single-view 3D Reconstruction Networks</papertitle>
              </a>
              <br>
              <strong>Yefan Zhou</strong>,
              <a href="https://scholar.google.com/citations?user=-_Hy9z0AAAAJ&hl=en">Yiru Shen</a>,
              <a href="https://people.eecs.berkeley.edu/~wguo/">Yujun Yan</a>,
              <a href="https://engineering.nyu.edu/faculty/chen-feng">Chen Feng</a>,
              <a href="https://sites.google.com/site/yangyaoqingcmu/">Yaoqing Yang</a>
              <br>
              <em>International Conference on 3D Vision (3DV)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2111.15158">arXiv</a> /
              <a href="https://github.com/YefanZhou/dispersion-score">Github</a> /
              <a href="https://3dv2021.surrey.ac.uk/accepted-papers/">3DV 2021</a> /
              <a href="https://slideslive.com/38972241/">Video</a>
              <p></p>
              <p>
                A SVR model can be disposed towards recognition (classification-based) or reconstruction depending on how dispersed the training data becomes.<br>
                We propose "dispersion score", which is a data-driven metric used to measure the tendency of SVR models to perform recognition or reconstruction. 
                It can also be used to diagnose problems from the training data and guide the design of data augmentation schemes.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/mlgsl.png' width="320"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Learn to Grasp with Less Supervision: A Data-Efficient Maximum Likelihood Grasp Sampling Loss</papertitle>
              <br>  
              <a href="https://rolandzhu.github.io/">Xinghao Zhu</a>,
              <strong>Yefan Zhou</strong>,
              <a href="https://scholar.google.com/citations?user=YAGWw68AAAAJ&hl=en">Yongxiang Fan</a>,
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/">Jianyu Chen</a>,
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/">Masayoshi Tomizuka</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2110.01379">arXiv</a> /
              <a href="https://www.icra2022.org/">ICRA 2022</a> /
              <a href="https://youtu.be/vHTMWdj4n7o">Video</a>
              <br>
              <p></p>
              <p>
                Empirical grasping datasets are typically sparsely labeled (i.e., a small number of successful grasp labels in each image). <br> We propose a maximum likelihood grasp sampling loss (MLGSL) for learning robotic grasping from sparsely labeled datasets. <br> MLGSL is 8× more data-efficient than SOTA with a 91.8% grasp success rate in real-world experiments.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/mf_gpd_pt.png' width="310"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              
              <papertitle>Multi-Fingered Grasp Pose Detection using Point Cloud</papertitle>
              
              <br>
              <a href="https://rolandzhu.github.io/">Xinghao Zhu</a>,
              <a href="https://scholar.google.com/citations?user=YAGWw68AAAAJ&hl=en">Yongxiang Fan</a>,
              <a href="https://changhaowang.github.io/">Changhao Wang</a>,
              <strong>Yefan Zhou</strong>,
              <a href="https://www.linkedin.com/in/shiyu-jin-320553124/">Shiyu Jin</a>,
              <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/">Masayoshi Tomizuka</a>
              <br>
              <em>Under Review, IEEE Robotics and Automation Letters (RAL) </em>, 2020
              <br>
              <a href="https://rolandzhu.github.io/MF-GPD/">Website</a> /
              <a href="https://youtu.be/lPpFQo0hPWI">Video</a>
              <p></p>
              <p>
                We propose a multi-fingered grasp pose detection (MF-GPD) algorithm to plan grasps in clutter. The algorithm generates grasp candidates using a cross-entropy sampler and assesses them with PointNet. <br> MF-GPD can locate a collision-free grasp in the clutter within 1 second with 72% success rate in real-world experiments.
              </p>
            </td>
          </tr>
          <!--
          <tr onmouseout="ic_stop()" onmouseover="ic_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='ic_red'>
                  <img src='images/ic.png' width="220"></div>
              <div class="one" id='ic_blue'>
                  <img src='images/ic_blue.png' width="250">
              </div>
                <script type="text/javascript">
                function ic_start() {
                  document.getElementById('ic_blue').style.opacity = "1";
                  document.getElementById('ic_red').style.opacity = "0";
                }
                function ic_stop() {
                  document.getElementById('ic_blue').style.opacity = "0";
                  document.getElementById('ic_red').style.opacity = "1";
                }
                ic_stop()
              </script>
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Integrative Complexity in Election Cycles: Lessons from American Presidential Candidates</papertitle>
              <br>
              <strong>Alex Zhao</strong>,
              <a href="https://jennifer-j-gebhardt.com/">Jennifer Jones</a>,
              <a href="https://www.faculty.uci.edu/profile.cfm?faculty_id=4880">Kristen Monroe</a>
              <br>
              <em>Oral paper at Annual Conference of the International Society on Political Psychology</em>, 2017
              <br>
              <a href="https://medium.com/@axyzhao/how-do-we-quantify-polarization-24a9d6c86deb">Medium post</a>
              <p></p>
              <p>
                Using the concept of integrative complexity, a psychometric developed to measure the degree of information processing capacity, this paper analyzes differences between Republican and Democratic presidential candidates on domestic and foreign policy by coding a set of interview transcripts (N = 77).
              </p>
            </td>
          </tr>
          -->
        
        </tbody></table>
        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="staff_stop()" onmouseover="staff_start()">
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>H-PG: Hybrid Deep Reinforcement Learning with Robotic Grasp Planning</papertitle>
              <br>
              <strong>Yefan Zhou</strong>, <a href="https://www.linkedin.com/in/xiangyu1998/">Xiangyu Zhou</a> and <a href="https://www.linkedin.com/in/jerry-ge-12b840124/">Jerry Ge</a>
              <br>
              <a href="https://www.youtube.com/watch?v=YFV3_CLpEdk">Video</a> /
              <a href="data/CS285_FinalProject_ICLR.pdf">PDF</a>
                <p>
                  We proposed Hybrid Policy Gradient (H-PG), a novel deep reinforcement learning framework for robotic grasping task defined in continuous-discrete hybrid action space; <br> H-PG improves baseline by 7.4% of grasp success rate on YCB dataset in PyBullet simulator.
                  <br><br>
                </p>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='cover'>
                  <img src='images/h_pg.png' width="350"></div>
              <div class="one" id='viz'>
                  <img src='images/h_pg_comp.png' width="380">
              </div>              <br>
              <script type="text/javascript">
              function staff_start() {
                document.getElementById('viz').style.opacity = "1";
                document.getElementById('cover').style.opacity = "0";
              }

              function staff_stop() {
                document.getElementById('viz').style.opacity = "0";
                document.getElementById('cover').style.opacity = "1";
              }
              staff_stop()
            </script>
            </td>
          </tr>

          

          
          <!--
          <tr onmouseout="bpd_stop()" onmouseover="bpd_start()">
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://projects.dailycal.org/2020/stops/"><papertitle>Police stops in Berkeley</papertitle></a>
              <br>
              <strong>Alex Zhao</strong>,
              <a href="https://www.dailycal.org/author/pkapshikar/">Purva Kapshikar</a>,
              <a href="https://www.dailycal.org/author/akatewa/">Aditya Katewa</a>,
              <a href="https://www.dailycal.org/author/chsu/">Catherine Hsu</a>
              <br>
              October 2020
                <p>
                  In this piece, we investigate police stops in the city of Berkeley. First, we delve into when and where stops happen in Berkeley. Next, we examine specific trends in traffic stops.
                </p>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="two" id='bar'>
                  <img src='images/bar.png' width="200"></div>
              <div class="one" id='car'>
                  <img src='images/bpd_car.jpg' width="250">
              </div>
                <script type="text/javascript">
                function bpd_start() {
                  document.getElementById('bar').style.opacity = "1";
                  document.getElementById('car').style.opacity = "0";
                }

                function bpd_stop() {
                  document.getElementById('bar').style.opacity = "0";
                  document.getElementById('car').style.opacity = "1";
                }
                bpd_stop()
              </script>
              <br>
            </td>
          </tr>
          -->

          

          <!--<tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://medium.com/@axyzhao/on-memory-and-self-810d839a0c92"><papertitle>On memory and self</papertitle></a>
              <br>
              <strong>Alex Zhao</strong>
              <br>
              June 2020
                <p>
                  All of us have memories we treasure and memories we loathe — vivid images of the past that, for better or worse, shape us to this day. Yet with the passage of time, memories warp and grow hazy. Understanding what is forgotten can tell us about who we are in the present, perhaps with even greater clarity than what is remembered.
                </p>
            </td>
            <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/clocks.jpg' width="250"></div>
              <br>
            </td>
          </tr>-->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <a href="https://jonbarron.info/">Website template</a>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
